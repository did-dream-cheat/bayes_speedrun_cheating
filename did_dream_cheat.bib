@article{GOODMAN2008135,
title = "A Dirty Dozen: Twelve P-Value Misconceptions",
journal = "Seminars in Hematology",
volume = "45",
number = "3",
pages = "135 - 140",
year = "2008",
note = "Interpretation of Quantitative Research",
issn = "0037-1963",
doi = "https://doi.org/10.1053/j.seminhematol.2008.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0037196308000620",
author = "Steven Goodman",
abstract = "The P value is a measure of statistical evidence that appears in virtually all medical research papers. Its interpretation is made extraordinarily difficult because it is not part of any formal system of statistical inference. As a result, the P value's inferential meaning is widely and often wildly misconstrued, a fact that has been pointed out in innumerable papers and books appearing since at least the 1940s. This commentary reviews a dozen of these common misinterpretations and explains why each is wrong. It also reviews the possible consequences of these improper understandings or representations of its meaning. Finally, it contrasts the P value with its Bayesian counterpart, the Bayes' factor, which has virtually all of the desirable properties of an evidential measure that the P value lacks, most notably interpretability. The most serious consequence of this array of P-value misconceptions is the false belief that the probability of a conclusion being in error can be calculated from the data in a single experiment without reference to external evidence or the plausibility of the underlying mechanism."
}
@article{gigerenzer1993superego,
  title={The superego, the ego, and the id in statistical reasoning},
  author={Gigerenzer, Gerd},
  journal={A handbook for data analysis in the behavioral sciences: Methodological issues},
  pages={311--339},
  year={1993}
}
@article{doi:10.7326/0003-4819-130-12-199906150-00008,
title = {Toward Evidence-Based Medical Statistics. 1: The P Value Fallacy},
journal = {Annals of Internal Medicine},
author = "Steven Goodman",
volume = {130},
number = {12},
pages = {995-1004},
year = {1999},
doi = {10.7326/0003-4819-130-12-199906150-00008},
note ={PMID: 10383371},
URL = {https://www.acpjournals.org/doi/abs/10.7326/0003-4819-130-12-199906150-00008},
eprint = {https://www.acpjournals.org/doi/pdf/10.7326/0003-4819-130-12-199906150-00008},
abstract = { An important problem exists in the interpretation of modern medical research data: Biological understanding and previous research play little formal role in the interpretation of quantitative results. This phenomenon is manifest in the discussion sections of research articles and ultimately can affect the reliability of conclusions. The standard statistical approach has created this situation by promoting the illusion that conclusions can be produced with certain “error rates,” without consideration of information from outside the experiment. This statistical approach, the key components of which are P values and hypothesis tests, is widely perceived as a mathematically coherent approach to inference. There is little appreciation in the medical community that the methodology is an amalgam of incompatible elements, whose utility for scientific inference has been the subject of intense debate among statisticians for almost 70 years. This article introduces some of the key elements of that debate and traces the appeal and adverse impact of this methodology to the P value fallacy, the mistaken idea that a single number can capture both the long-run outcomes of an experiment and the evidential meaning of a single result. This argument is made as a prelude to the suggestion that another measure of evidence should be used—the Bayes factor, which properly separates issues of long-run behavior from evidential strength and allows the integration of background knowledge with statistical findings. }
}
@article{cohen_earth_1994,
	title = {The earth is round (p {\textless} .05)},
	volume = {49},
	copyright = {(c) 2012 APA, all rights reserved},
	issn = {1935-990X(Electronic);0003-066X(Print)},
	doi = {10.1037/0003-066X.49.12.997},
	abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
	number = {12},
	journal = {American Psychologist},
	author = {Cohen, Jacob},
	year = {1994},
	keywords = {Null Hypothesis Testing},
	pages = {997--1003},
}
@article{vanderplas_frequentism_2014,
	title = {Frequentism and {Bayesianism}: {A} {Python}-driven {Primer}},
	shorttitle = {Frequentism and {Bayesianism}},
	url = {http://arxiv.org/abs/1411.5018},
	abstract = {This paper presents a brief, semi-technical comparison of the essential features of the frequentist and Bayesian approaches to statistical inference, with several illustrative examples implemented in Python. The differences between frequentism and Bayesianism fundamentally stem from differing definitions of probability, a philosophical divide which leads to distinct approaches to the solution of statistical problems as well as contrasting ways of asking and answering questions about unknown parameters. After an example-driven discussion of these differences, we briefly compare several leading Python statistical packages which implement frequentist inference using classical methods and Bayesian inference using Markov Chain Monte Carlo.},
	urldate = {2015-12-17},
	journal = {arXiv:1411.5018 [astro-ph]},
	author = {VanderPlas, Jake},
	month = nov,
	year = {2014},
	note = {00000 
arXiv: 1411.5018},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
}
@article{fisher1934statistical,
  title={Statistical methods for research workers.},
  author={Fisher, Ronald Aylmer and others},
  journal={Statistical methods for research workers.},
  number={5th Ed},
  year={1934},
  publisher={Oliver and Boyd, Edinburgh and London}
}
@article{doi:10.1111/j.1745-6924.2008.00079.x,
author = {Geoff Cumming},
title ={Replication and p Intervals: p Values Predict the Future Only Vaguely, but Confidence Intervals Do Much Better},
journal = {Perspectives on Psychological Science},
volume = {3},
number = {4},
pages = {286-300},
year = {2008},
doi = {10.1111/j.1745-6924.2008.00079.x},
note ={PMID: 26158948},
URL = {https://doi.org/10.1111/j.1745-6924.2008.00079.x},
eprint = {https://doi.org/10.1111/j.1745-6924.2008.00079.x},
abstract = { Replication is fundamental to science, so statistical analysis should give information about replication. Because p values dominate statistical analysis in psychology, it is important to ask what p says about replication. The answer to this question is “Surprisingly little.” In one simulation of 25 repetitions of a typical experiment, p varied from <.001 to .76, thus illustrating that p is a very unreliable measure. This article shows that, if an initial experiment results in two-tailed p = .05, there is an 80\% chance the one-tailed p value from a replication will fall in the interval (.00008, .44), a 10\% chance that p <.00008, and fully a 10\% chance that p >.44. Remarkably, the interval—termed a p interval—is this wide however large the sample size. p is so unreliable and gives such dramatically vague information that it is a poor basis for inference. Confidence intervals, however, give much better information about replication. Researchers should minimize the role of p by using confidence intervals and model-fitting techniques and by adopting meta-analytic thinking. }
}
@article{hubbard_why_2008,
	title = {Why {P} {Values} {Are} {Not} a {Useful} {Measure} of {Evidence} in {Statistical} {Significance} {Testing}},
	volume = {18},
	issn = {0959-3543},
	url = {https://doi.org/10.1177/0959354307086923},
	doi = {10.1177/0959354307086923},
	abstract = {Reporting p values from statistical significance tests is common in psychology's empirical literature. Sir Ronald Fisher saw the p value as playing a useful role in knowledge development by acting as an `objective' measure of inductive evidence against the null hypothesis. We review several reasons why the p value is an unobjective and inadequate measure of evidence when statistically testing hypotheses. A common theme throughout many of these reasons is that p values exaggerate the evidence against H0. This, in turn, calls into question the validity of much published work based on comparatively small, including .05, p values. Indeed, if researchers were fully informed about the limitations of the  p value as a measure of evidence, this inferential index could not possibly enjoy its ongoing ubiquity. Replication with extension research focusing on sample statistics, effect sizes, and their confidence intervals is a better vehicle for reliable knowledge development than using p values. Fisher would also have agreed with the need for replication research.},
	language = {en},
	number = {1},
	urldate = {2020-07-24},
	journal = {Theory \& Psychology},
	author = {Hubbard, Raymond and Lindsay, R. Murray},
	month = feb,
	year = {2008},
	note = {Publisher: SAGE Publications Ltd},
	pages = {69--88},
}
@article{halsey_fickle_2015,
	title = {The fickle {P} value generates irreproducible results},
	volume = {12},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3288/},
	doi = {10.1038/nmeth.3288},
	abstract = {The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.},
	language = {en},
	number = {3},
	urldate = {2020-12-31},
	journal = {Nature Methods},
	author = {Halsey, Lewis G. and Curran-Everett, Douglas and Vowler, Sarah L. and Drummond, Gordon B.},
	month = mar,
	year = {2015},
	pages = {179--185},
}
@article{colquhoun2014investigation,
  title={An investigation of the false discovery rate and the misinterpretation of p-values},
  author={Colquhoun, David},
  journal={Royal Society open science},
  volume={1},
  number={3},
  pages={140216},
  year={2014},
  publisher={The Royal Society Publishing}
}
@article {Johnson19313,
	author = {Johnson, Valen E.},
	title = {Revised standards for statistical evidence},
	volume = {110},
	number = {48},
	pages = {19313--19317},
	year = {2013},
	doi = {10.1073/pnas.1313476110},
	publisher = {National Academy of Sciences},
	abstract = {The lack of reproducibility of scientific research undermines public confidence in science and leads to the misuse of resources when researchers attempt to replicate and extend fallacious research findings. Using recent developments in Bayesian hypothesis testing, a root cause of nonreproducibility is traced to the conduct of significance tests at inappropriately high levels of significance. Modifications of common standards of evidence are proposed to reduce the rate of nonreproducibility of scientific research by a factor of 5 or greater.Recent advances in Bayesian hypothesis testing have led to the development of uniformly most powerful Bayesian tests, which represent an objective, default class of Bayesian hypothesis tests that have the same rejection regions as classical significance tests. Based on the correspondence between these two classes of tests, it is possible to equate the size of classical hypothesis tests with evidence thresholds in Bayesian tests, and to equate P values with Bayes factors. An examination of these connections suggest that recent concerns over the lack of reproducibility of scientific studies can be attributed largely to the conduct of significance tests at unjustifiably high levels of significance. To correct this problem, evidence thresholds required for the declaration of a significant finding should be increased to 25{\textendash}50:1, and to 100{\textendash}200:1 for the declaration of a highly significant finding. In terms of classical hypothesis tests, these evidence standards mandate the conduct of tests at the 0.005 or 0.001 level of significance.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/110/48/19313},
	eprint = {https://www.pnas.org/content/110/48/19313.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
@inbook{doi:https://doi.org/10.1002/0471667196.ess0845.pub3,
author = {Griffiths, David},
publisher = {American Cancer Society},
isbn = {9780471667193},
title = {Statistics in Gambling},
booktitle = {Encyclopedia of Statistical Sciences},
chapter = {},
pages = {1-4},
doi = {https://doi.org/10.1002/0471667196.ess0845.pub3},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471667196.ess0845.pub3},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471667196.ess0845.pub3},
year = {2012}
}
@article{lyons_discovering_2013,
	title = {Discovering the {Significance} of 5 sigma},
	url = {http://arxiv.org/abs/1310.1284},
	abstract = {We discuss the traditional criterion for discovery in Particle Physics of requiring a significance corresponding to at least 5 sigma; and whether a more nuanced approach might be better.},
	urldate = {2021-01-14},
	journal = {arXiv:1310.1284 [hep-ex, physics:hep-ph, physics:physics]},
	author = {Lyons, Louis},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.1284},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability},
}
@article {Storey9440,
	author = {Storey, John D. and Tibshirani, Robert},
	title = {Statistical significance for genomewide studies},
	volume = {100},
	number = {16},
	pages = {9440--9445},
	year = {2003},
	doi = {10.1073/pnas.1530509100},
	publisher = {National Academy of Sciences},
	abstract = {With the increase in genomewide experiments and the sequencing of multiple genomes, the analysis of large data sets has become commonplace in biology. It is often the case that thousands of features in a genomewide data set are tested against some null hypothesis, where a number of features are expected to be significant. Here we propose an approach to measuring statistical significance in these genomewide studies based on the concept of the false discovery rate. This approach offers a sensible balance between the number of true and false positives that is automatically calibrated and easily interpreted. In doing so, a measure of statistical significance called the q value is associated with each tested feature. The q value is similar to the well known p value, except it is a measure of significance in terms of the false discovery rate rather than the false positive rate. Our approach avoids a flood of false positive results, while offering a more liberal criterion than what has been used in genome scans for linkage.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/100/16/9440},
	eprint = {https://www.pnas.org/content/100/16/9440.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
@article {ClarkE306,
	author = {Clark, Matthew P.A. and Westerberg, Brian D.},
	title = {How random is the toss of a coin?},
	volume = {181},
	number = {12},
	pages = {E306--E308},
	year = {2009},
	doi = {10.1503/cmaj.091733},
	publisher = {CMAJ},
	abstract = {Background: The toss of a coin has been a method used to determine random outcomes for centuries. It is still used in some research studies as a method of randomization, although it has largely been discredited as a valid randomization method. We sought to provide evidence that the toss of a coin can be manipulated. Methods: We performed a prospective experiment involving otolaryngology residents in Vancouver, Canada. The main outcome was the proportion of {\textquotedblleft}heads{\textquotedblright} coin tosses achieved (out of 300 attempts) by each participant. Each of the participants attempted to flip the coin so as to achieve a heads result. Results: All participants achieved more heads than tails results, with 7 of the 13 participants having significantly more heads results (p <= 0.05). The highest proportion of heads achieved was 0.68 (95\% confidence interval 0.62{\textendash}0.73, p \&lt; 0.001). Interpretation: Certain people are able to successfully manipulate the toss of a coin. This throws into doubt the validity of using a coin toss to determine a chance result.},
	issn = {0820-3946},
	URL = {https://www.cmaj.ca/content/181/12/E306},
	eprint = {https://www.cmaj.ca/content/181/12/E306.full.pdf},
	journal = {CMAJ}
}
@article{fink1997compendium,
  title={A compendium of conjugate priors},
  author={Fink, Daniel},
  journal={See http://www. people. cornell. edu/pages/df36/CONJINTRnew\% 20TEX. pdf},
  volume={46},
  year={1997},
  publisher={Citeseer}
}
@article{syversveen1998noninformative,
  title={Noninformative bayesian priors. interpretation and problems with construction and applications},
  author={Syversveen, Anne Randi},
  year={1998},
  journal={preprint},
  publisher={preprint}
}
@article{altman_standard_2005,
	title = {Standard deviations and standard errors},
	volume = {331},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1255808/},
	number = {7521},
	urldate = {2021-01-08},
	journal = {BMJ : British Medical Journal},
	author = {Altman, Douglas G and Bland, J Martin},
	month = oct,
	year = {2005},
	pmid = {16223828},
	pmcid = {PMC1255808},
	pages = {903},
}
@article{doi:10.3102/0013189X020009013,
author = {David B. Pillemer},
title ={One- Versus Two-Tailed Hypothesis Tests in Contemporary Educational Research},
journal = {Educational Researcher},
volume = {20},
number = {9},
pages = {13-17},
year = {1991},
doi = {10.3102/0013189X020009013},

URL = { 
        https://doi.org/10.3102/0013189X020009013
    
},
eprint = { 
        https://doi.org/10.3102/0013189X020009013
    
}
,
    abstract = { The choice of a one- rather than a two-tailed hypothesis testing strategy can influence research outcomes, but information about the type of test conducted is rarely reported in articles appearing in educational and psychological journals. Because unambiguous standards for using one- and two-tailed tests do not exist, complete reporting of hypothesis testing procedures is essential. In addition, educational researchers need to reevaluate the decision-oriented, “critical experiment” model of science that underlies the use of one-tailed tests. It is the adherence to the arbitrary .05 level of significance as a benchmark for publication decisions, rather than logical or methodological considerations, that largely accounts for the popularity of one-tailed tests. Effect size estimates, accompanied by confidence intervals or exact two-tailed probabilities, are generally more compatible with the growing meta-analytic view of social science as an incremental, cumulative, and shared enterprise. }
}
@article{frick_better_1998,
	title = {A better stopping rule for conventional statistical tests},
	volume = {30},
	issn = {1532-5970},
	url = {https://doi.org/10.3758/BF03209488},
	doi = {10.3758/BF03209488},
	abstract = {The goal of some research studies is to demonstrate the existence of an effect. Statistical testing, withp less than .05, is one criterion for establishing the existence of this effect. In this situation, the fixedsample stopping rule, in which the number of subjects is determined in advance, is impractical and inefficient. This article presents a sequential stopping rule that is practical and about 30\% more efficient: Once a minimum number of subjects is tested, stop withp less than .01 or greater than .36; otherwise, keep testing. This procedure keeps alpha at .05 and can be adjusted to fit researchers’ needs and inclinations.},
	language = {en},
	number = {4},
	urldate = {2021-01-12},
	journal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Frick, Robert W.},
	month = dec,
	year = {1998},
	pages = {690--697},
}
@article{choi_new_2010,
	title = {A {New} {Stopping} {Rule} for {Computerized} {Adaptive} {Testing}},
	volume = {70},
	issn = {0013-1644},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3028267/},
	doi = {10.1177/0013164410387338},
	abstract = {The goal of the current study was to introduce a new stopping rule for computerized adaptive testing. The predicted standard error reduction stopping rule (PSER) uses the predictive posterior variance to determine the reduction in standard error that would result from the administration of additional items. The performance of the PSER was compared to that of the minimum standard error stopping rule and a modified version of the minimum information stopping rule in a series of simulated adaptive tests, drawn from a number of item pools. Results indicate that the PSER makes efficient use of CAT item pools, administering fewer items when predictive gains in information are small and increasing measurement precision when information is abundant.},
	number = {6},
	urldate = {2021-01-12},
	journal = {Educational and psychological measurement},
	author = {Choi, Seung W. and Grady, Matthew W. and Dodd, Barbara G.},
	month = dec,
	year = {2010},
	pmid = {21278821},
	pmcid = {PMC3028267},
	pages = {1--17},
}
@article{pocock_when_1992,
	title = {When to stop a clinical trial.},
	volume = {305},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1882707/},
	abstract = {Images
null},
	number = {6847},
	urldate = {2021-01-12},
	journal = {BMJ : British Medical Journal},
	author = {Pocock, S. J.},
	month = jul,
	year = {1992},
	pmid = {1392832},
	pmcid = {PMC1882707},
	pages = {235--240},
}
@article{doi:10.1080/00031305.2018.1527253,
author = {Blakeley B. McShane and David Gal and Andrew Gelman and Christian Robert and Jennifer L. Tackett},
title = {Abandon Statistical Significance},
journal = {The American Statistician},
volume = {73},
number = {sup1},
pages = {235-245},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2018.1527253},
URL = {https://doi.org/10.1080/00031305.2018.1527253},
eprint = {https://doi.org/10.1080/00031305.2018.1527253}
}

