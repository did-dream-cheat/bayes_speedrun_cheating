
@inproceedings{goodman_dirty_2008,
	title = {A dirty dozen: twelve p-value misconceptions},
	volume = {45},
	shorttitle = {A dirty dozen},
	url = {http://www.sciencedirect.com/science/article/pii/S0037196308000620},
	urldate = {2015-12-13},
	booktitle = {Seminars in hematology},
	publisher = {Elsevier},
	author = {Goodman, Steven},
	year = {2008},
	note = {00111},
	pages = {135--140},
}
@article{cohen_earth_1994,
	title = {The earth is round (p {\textless} .05)},
	volume = {49},
	copyright = {(c) 2012 APA, all rights reserved},
	issn = {1935-990X(Electronic);0003-066X(Print)},
	doi = {10.1037/0003-066X.49.12.997},
	abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
	number = {12},
	journal = {American Psychologist},
	author = {Cohen, Jacob},
	year = {1994},
	keywords = {Null Hypothesis Testing},
	pages = {997--1003},
}

@article{vanderplas_frequentism_2014,
	title = {Frequentism and {Bayesianism}: {A} {Python}-driven {Primer}},
	shorttitle = {Frequentism and {Bayesianism}},
	url = {http://arxiv.org/abs/1411.5018},
	abstract = {This paper presents a brief, semi-technical comparison of the essential features of the frequentist and Bayesian approaches to statistical inference, with several illustrative examples implemented in Python. The differences between frequentism and Bayesianism fundamentally stem from differing definitions of probability, a philosophical divide which leads to distinct approaches to the solution of statistical problems as well as contrasting ways of asking and answering questions about unknown parameters. After an example-driven discussion of these differences, we briefly compare several leading Python statistical packages which implement frequentist inference using classical methods and Bayesian inference using Markov Chain Monte Carlo.},
	urldate = {2015-12-17},
	journal = {arXiv:1411.5018 [astro-ph]},
	author = {VanderPlas, Jake},
	month = nov,
	year = {2014},
	note = {00000 
arXiv: 1411.5018},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
}
@article{fisher1934statistical,
  title={Statistical methods for research workers.},
  author={Fisher, Ronald Aylmer and others},
  journal={Statistical methods for research workers.},
  number={5th Ed},
  year={1934},
  publisher={Oliver and Boyd, Edinburgh and London}
}
@article{doi:10.1111/j.1745-6924.2008.00079.x,
author = {Geoff Cumming},
title ={Replication and p Intervals: p Values Predict the Future Only Vaguely, but Confidence Intervals Do Much Better},
journal = {Perspectives on Psychological Science},
volume = {3},
number = {4},
pages = {286-300},
year = {2008},
doi = {10.1111/j.1745-6924.2008.00079.x},
note ={PMID: 26158948},
URL = {https://doi.org/10.1111/j.1745-6924.2008.00079.x},
eprint = {https://doi.org/10.1111/j.1745-6924.2008.00079.x},
abstract = { Replication is fundamental to science, so statistical analysis should give information about replication. Because p values dominate statistical analysis in psychology, it is important to ask what p says about replication. The answer to this question is “Surprisingly little.” In one simulation of 25 repetitions of a typical experiment, p varied from <.001 to .76, thus illustrating that p is a very unreliable measure. This article shows that, if an initial experiment results in two-tailed p = .05, there is an 80\% chance the one-tailed p value from a replication will fall in the interval (.00008, .44), a 10\% chance that p <.00008, and fully a 10\% chance that p >.44. Remarkably, the interval—termed a p interval—is this wide however large the sample size. p is so unreliable and gives such dramatically vague information that it is a poor basis for inference. Confidence intervals, however, give much better information about replication. Researchers should minimize the role of p by using confidence intervals and model-fitting techniques and by adopting meta-analytic thinking. }
}
@article{hubbard_why_2008,
	title = {Why {P} {Values} {Are} {Not} a {Useful} {Measure} of {Evidence} in {Statistical} {Significance} {Testing}},
	volume = {18},
	issn = {0959-3543},
	url = {https://doi.org/10.1177/0959354307086923},
	doi = {10.1177/0959354307086923},
	abstract = {Reporting p values from statistical significance tests is common in psychology's empirical literature. Sir Ronald Fisher saw the p value as playing a useful role in knowledge development by acting as an `objective' measure of inductive evidence against the null hypothesis. We review several reasons why the p value is an unobjective and inadequate measure of evidence when statistically testing hypotheses. A common theme throughout many of these reasons is that p values exaggerate the evidence against H0. This, in turn, calls into question the validity of much published work based on comparatively small, including .05, p values. Indeed, if researchers were fully informed about the limitations of the  p value as a measure of evidence, this inferential index could not possibly enjoy its ongoing ubiquity. Replication with extension research focusing on sample statistics, effect sizes, and their confidence intervals is a better vehicle for reliable knowledge development than using p values. Fisher would also have agreed with the need for replication research.},
	language = {en},
	number = {1},
	urldate = {2020-07-24},
	journal = {Theory \& Psychology},
	author = {Hubbard, Raymond and Lindsay, R. Murray},
	month = feb,
	year = {2008},
	note = {Publisher: SAGE Publications Ltd},
	pages = {69--88},
}
@article{halsey_fickle_2015,
	title = {The fickle {P} value generates irreproducible results},
	volume = {12},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3288/},
	doi = {10.1038/nmeth.3288},
	abstract = {The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.},
	language = {en},
	number = {3},
	urldate = {2020-12-31},
	journal = {Nature Methods},
	author = {Halsey, Lewis G. and Curran-Everett, Douglas and Vowler, Sarah L. and Drummond, Gordon B.},
	month = mar,
	year = {2015},
	pages = {179--185},
}
@article{colquhoun2014investigation,
  title={An investigation of the false discovery rate and the misinterpretation of p-values},
  author={Colquhoun, David},
  journal={Royal Society open science},
  volume={1},
  number={3},
  pages={140216},
  year={2014},
  publisher={The Royal Society Publishing}
}
@article {Johnson19313,
	author = {Johnson, Valen E.},
	title = {Revised standards for statistical evidence},
	volume = {110},
	number = {48},
	pages = {19313--19317},
	year = {2013},
	doi = {10.1073/pnas.1313476110},
	publisher = {National Academy of Sciences},
	abstract = {The lack of reproducibility of scientific research undermines public confidence in science and leads to the misuse of resources when researchers attempt to replicate and extend fallacious research findings. Using recent developments in Bayesian hypothesis testing, a root cause of nonreproducibility is traced to the conduct of significance tests at inappropriately high levels of significance. Modifications of common standards of evidence are proposed to reduce the rate of nonreproducibility of scientific research by a factor of 5 or greater.Recent advances in Bayesian hypothesis testing have led to the development of uniformly most powerful Bayesian tests, which represent an objective, default class of Bayesian hypothesis tests that have the same rejection regions as classical significance tests. Based on the correspondence between these two classes of tests, it is possible to equate the size of classical hypothesis tests with evidence thresholds in Bayesian tests, and to equate P values with Bayes factors. An examination of these connections suggest that recent concerns over the lack of reproducibility of scientific studies can be attributed largely to the conduct of significance tests at unjustifiably high levels of significance. To correct this problem, evidence thresholds required for the declaration of a significant finding should be increased to 25{\textendash}50:1, and to 100{\textendash}200:1 for the declaration of a highly significant finding. In terms of classical hypothesis tests, these evidence standards mandate the conduct of tests at the 0.005 or 0.001 level of significance.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/110/48/19313},
	eprint = {https://www.pnas.org/content/110/48/19313.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
@inbook{doi:https://doi.org/10.1002/0471667196.ess0845.pub3,
author = {Griffiths, David},
publisher = {American Cancer Society},
isbn = {9780471667193},
title = {Statistics in Gambling},
booktitle = {Encyclopedia of Statistical Sciences},
chapter = {},
pages = {1-4},
doi = {https://doi.org/10.1002/0471667196.ess0845.pub3},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471667196.ess0845.pub3},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471667196.ess0845.pub3},
year = {2012}
}